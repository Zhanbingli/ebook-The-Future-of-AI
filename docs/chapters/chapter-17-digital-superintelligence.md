# Elon Musk: Digital Superintelligence, Multiplanetary Life, How to Be Useful

- **Date:** June 19, 2025
- **Source:** [Youtube](https://www.youtube.com/watch?v=cFIlta1GkiE)

We're at the very early stages of the intelligence big bang. Being a multiplanetary species greatly increases the probable lifespan of civilization—of consciousness or intelligence, both biological and digital. I think we're quite close to digital superintelligence. If it doesn't happen this year, next year for sure.

Please give it up for Elon Musk.

**Moderator:** Elon, welcome to AI Startup School. We're really blessed to have your presence here today.

**Elon:** Thanks for having me.

**Moderator:** From SpaceX, Tesla, Neuralink, xAI, and more—was there ever a moment in your life before all this where you felt "I have to build something great"? What flipped that switch for you?

**Elon:** Well, I didn't originally think I would build something great. I wanted to try to build something useful, but I didn't think I would build anything particularly great. Probabilistically, it seemed unlikely, but I wanted to at least try.

**Moderator:** You're talking to a room full of people who are all technical engineers—often some of the most eminent AI researchers coming up in the game.

**Elon:** I think I like the term "engineer" better than "researcher." I suppose if there's some fundamental algorithmic breakthrough, it's research. But otherwise, it's engineering.

## Early Days and First Principles

**Moderator:** Let's start way back. This is a room full of 18-to-25-year-olds—it skews younger because the founder demographic keeps getting younger. Can you put yourself back in their shoes when you were 18 or 19, learning to code, coming up with your first idea for Zip2? What was that like?

**Elon:** Back in '95, I was faced with a choice: either pursue graduate studies—a PhD at Stanford in materials science, working on ultracapacitors for potential use in electric vehicles, essentially trying to solve the range problem for EVs—or try to do something in this thing that most people had never heard of called the internet.

I talked to my professor, Bill Nix in materials science, and said, "Can I defer for a quarter because this will probably fail and then I'll need to come back to college?" He said, "This is probably the last conversation we'll have"—and he was right.

I thought things would most likely fail, not succeed. In '95, I wrote basically the first—or close to the first—maps, directions, internet white pages and yellow pages on the internet. I wrote it personally and didn't even use a web server. I just read the port directly because I couldn't afford a T1 connection.

Our original office was on Sherman Avenue in Palo Alto. There was an ISP on the floor below, so I drilled a hole through the floor and ran a LAN cable directly to the ISP. My brother joined me, along with another co-founder, Greg Couri, who passed away. At the time, we couldn't even afford a place to stay, so we slept in the office—it was 500 bucks a month—and showered at the YMCA on Page Mill Road.

We ended up building something useful with Zip2. We built really good software technology, but we were somewhat captured by legacy media companies. The New York Times and others were investors, customers, and board members. They kept wanting to use our software in ways that made no sense. I wanted to go direct to consumers.

The long story short is that I really just wanted to do something useful on the internet. I had two choices: do a PhD and watch people build the internet, or help build the internet in some small way. I figured I could always try, fail, and then go back to graduate studies.

It ended up being reasonably successful. We sold for $300 million, which was a lot at the time. These days, that's like the minimum impulse bid for an AI startup—a billion dollars. There are so many unicorns now, it's like a herd of unicorns.

## Taking Risks and Rolling Forward

**Moderator:** You solved the money problem, at least. You basically took your $20 million and kept rolling with X.com, which became PayPal. Not everyone does that. What drove you to jump back into the ring?

**Elon:** I felt that with Zip2, we'd built incredible technology, but it never really got used. From my perspective, we had better technology than Yahoo or anyone else, but it was constrained by our customers. I wanted to do something where we wouldn't be constrained by customers—go direct to consumer. That's what ended up being X.com/PayPal.

X.com merged with Confinity, which together created PayPal. The PayPal diaspora has probably created more companies than anything else in the 21st century. So many talented people were at that combination of Confinity and X.com.

I felt like we got our wings clipped with Zip2. What if our wings aren't clipped and we go direct to consumer? That's what PayPal ended up being.

When I got that $20 million check for my share of Zip2, I was living in a house with four housemates and had maybe 10 grand in the bank. Then this check arrives in the mail, and my bank balance went from $10,000 to $20,010,000. You're like, "Well, okay." Still had to pay taxes and all, but I ended up putting almost all of that into X.com—keeping almost all the chips on the table.

## The Mars Question

After PayPal, I was curious about why we hadn't sent anyone to Mars. I went to NASA's website to find out when we're sending people to Mars, and there was no date. I thought maybe it was just hard to find on the website, but there was no real plan to send people to Mars.

I was on the Long Island Expressway with my friend Adeo Ressi—we were housemates in college—and he was asking what I was going to do after PayPal. I thought maybe I'd like to do something philanthropic in space because I didn't think I could do anything commercial in space—that seemed like the purview of nations.

My first idea was a philanthropic mission to Mars called "Life to Mars," where we'd send a small greenhouse with seeds and dehydrated nutrient gel, land it on Mars, grow plants, and you'd have this great shot of green plants on a red background to inspire NASA and the public to send astronauts to Mars.

Along the way, I went to Russia in 2001 and 2002 to buy ICBMs—which is quite an adventure. You meet with Russian high command and say, "I'd like to buy some ICBMs." This was to get to space, not to nuke anyone. They had to destroy missiles due to arms reduction talks, so I thought, "What if we take two of those, minus the nukes, add an upper stage for Mars?"

It was trippy being in Moscow negotiating with the Russian military to buy ICBMs. But they kept raising the price—literally the opposite of what a negotiation should do. These things were getting really expensive.

I came to realize that the problem wasn't insufficient will to go to Mars, but that there was no way to do it without breaking NASA's budget. That's when I decided to start SpaceX—to advance rocket technology to the point where we could send people to Mars.

## The SpaceX Gambit

**Elon:** That was in 2002. I didn't start out wanting to build a business—I wanted to do something humanity needed. Then, like a cat pulling on a string, the ball unravels and it turns out this could be a very profitable business.

There had been no prior example of a rocket startup succeeding. Various attempts at commercial rocket companies had all failed. Starting SpaceX was from the standpoint of maybe a less than 10% chance of success—maybe 1%.

But if a startup doesn't do something to advance rocket technology, it's definitely not coming from big defense contractors because they just have cost-plus contracts with the government, and the government wants to do very conventional things. It's either coming from a startup or it's not happening at all. A small chance of success is better than no chance of success.

I started SpaceX in mid-2002 expecting to fail—probably a 90% chance of failing. When recruiting people, I didn't try to make it sound good. I said, "We're probably going to die, but there's a 10% chance we might not die, and this is the only way to get people to Mars and advance the state of the art."

I ended up being chief engineer of the rocket—not because I wanted to, but because I couldn't hire anyone good. None of the good chief engineers would join because they thought it was too risky.

The first three flights failed. The fourth one fortunately worked. If the fourth one hadn't worked, I had no money left and that would have been curtains. We would have joined the graveyard of prior rocket startups. My estimate of success wasn't far off—we made it by the skin of our teeth.

Tesla was happening simultaneously. 2008 was a rough year. By mid-2008, the third SpaceX launch had failed—our third failure in a row. The Tesla financing round had failed, and Tesla was going bankrupt fast. It was grim. This was going to be a tale of warning, an exercise in hubris.

Throughout that period, people were saying, "Elon is a software guy. Why is he working on hardware?" You can still search the press from that time—they kept calling me "internet guy." "Internet guy, aka fool, is attempting to build a rocket company." We got ridiculed quite a lot.

It does sound absurd—internet guy starts rocket company doesn't sound like a recipe for success, frankly. I don't hold it against them. It admittedly sounds improbable, and I agree it's improbable.

Fortunately, the fourth launch worked, and NASA awarded us a contract to resupply the space station. That was maybe December 22nd, right before Christmas. Even the fourth launch working wasn't enough to succeed—we also needed a big contract to keep us alive.

I got that call from the NASA team. They said, "We're awarding you one of the contracts to resupply the space station." I literally blurted out, "I love you guys!"—which is not normally what they hear. It's usually pretty sober, but I was like, "Man, this is a company saver."

Then we closed the Tesla financing round on the last hour of the last day possible—6 p.m., December 24th, 2008. We would have bounced payroll two days after Christmas if that round hadn't closed. That was a nerve-wracking end to 2008.

## Being Useful

**Moderator:** From your PayPal and Zip2 experience, jumping into hardcore hardware startups, one through line was being able to find and attract the smartest possible people in those fields. Most people here haven't even managed a single person yet. What would you tell the Elon who never had to do that?

**Elon:** I generally think: try to be as useful as possible. It may sound trite, but it's so hard to be useful, especially to be useful to a lot of people. The area under the curve of total utility—how useful have you been to your fellow human beings times how many people—is almost like the physics definition of work. It's incredibly difficult to do that.

If you aspire to do true work, your probability of success is much higher. Don't aspire to glory—aspire to work.

**Moderator:** How can you tell it's true work?

**Elon:** In terms of your end product, you have to ask: if this thing is successful, how useful will it be to how many people? Then you do whatever it takes to succeed—whether you're CEO or any role in a startup. Always be smashing your ego. Internalize responsibility.

A major failure mode is when your ego-to-ability ratio gets too high. You'll break the feedback loop to reality. In AI terms, you'll break your RL loop. You want a strong RL loop, which means internalizing responsibility, minimizing ego, and doing whatever the task is, no matter whether it's grand or humble.

That's why I prefer the term "engineering" over "research." I don't want to call xAI a "lab"—I just want it to be a company. Whatever the simplest, most straightforward, ideally lowest-ego terms are, those are generally good. You want to close the loop on reality hard.

## First Principles Thinking

**Moderator:** Everyone in this room looks up to your approach to first principles thinking. How do you actually determine reality? You have critics who've never built anything, but you also have builders with high "area under the curve" in your circle. How should people approach that?

**Elon:** The tools of physics are incredibly helpful for understanding and making progress in any field. First principles means breaking things down to the fundamental axiomatic elements that are most likely to be true, then reasoning up from there as cogently as possible—as opposed to reasoning by analogy or metaphor.

Simple things like thinking in the limit—if you extrapolate, minimize this thing or maximize that thing—thinking in the limit is very helpful. I use all the tools of physics. They apply to any field. This is like a superpower.

Take rockets, for example. How much should a rocket cost? The typical approach is to look historically at rocket costs and assume any new rocket must be similar. A first principles approach would be to look at the materials the rocket comprises—aluminum, copper, carbon fiber, steel—and ask what the rocket weighs, what the constituent elements are, and what the material price per kilogram is. That sets the actual floor on what a rocket can cost. It can asymptotically approach the cost of raw materials.

You realize that raw materials of a rocket are only maybe 1-2% of the historical cost of a rocket. So manufacturing must be very inefficient if raw material cost is only 1-2%. That's a first principles analysis of the potential for cost optimization—and that's before you get to reusability.

## Building the Training Supercluster

To give an AI example: last year with xAI, when we were trying to build a training supercluster, we went to various suppliers and said we needed 100,000 H100s to train coherently. Their estimates for completion were 18-24 months. We needed it done in six months or we wouldn't be competitive.

If you break that down: What do you need? You need a building, power, cooling. We didn't have time to build from scratch, so we found a factory in Memphis that used to build Electrolux products. But the input power was 15 megawatts and we needed 150 megawatts.

We rented generators and put them on one side of the building. We rented about a quarter of the mobile cooling capacity of the US and put chillers on the other side. That didn't fully solve the problem because power variations during training are huge—power can drop 50% in 100 milliseconds, which generators can't keep up with. So we added Tesla Megapacks and modified their software to smooth out power variations during training.

Then there were networking challenges. The networking cables to make 100,000 GPUs train coherently are very challenging. We ran the networking operation in four shifts, 24/7. I was sleeping in the data center and doing cabling myself.

Nobody had done a training run with 100,000 H100s training coherently last year. Maybe it's been done this year. We ended up doubling that to 200,000. Now we have 150,000 H100s, 50,000 H200s, and 30,000 GB200s in the Memphis training center. We're about to bring 110,000 GB200s online at a second data center, also in the Memphis area.

## The AI Race and Truth

**Moderator:** Is it your view that pre-training is still working, scaling laws still hold, and whoever wins this race will have the biggest, smartest model?

**Elon:** There are various elements besides competitiveness for large AI. The talent of people matters. The scale of hardware matters, and how well you're able to bring that hardware to bear. You can't just order GPUs and plug them in. You've got to get them to train coherently and stably.

What unique access to data do you have? Distribution matters—how do people get exposed to your AI? These are critical factors for a competitive large foundation model.

We've kind of run out of pre-training data—human-generated, high-quality tokens. You need to essentially create synthetic data and be able to accurately judge whether it's real synthetic data or hallucination that doesn't match reality. Achieving grounding in reality is tricky, but we're at the stage where there's more effort put into synthetic data.

Right now we're training Grok 3.5, which has a heavy focus on reasoning.

**Moderator:** Going back to your physics point—I heard that hard science, particularly physics textbooks, are very useful for reasoning, whereas social science is totally useless for reasoning.

**Elon:** Yes, that's probably true.

## Robotics and the Future

Something that's going to be very important is combining deep AI in data centers with robotics—things like the Optimus humanoid robot. Optimus is awesome. There are going to be so many humanoid robots of all sizes and shapes, but my prediction is there will be more humanoid robots by far than all other robots combined—maybe by an order of magnitude.

**Moderator:** Is it true you're planning a robot army of sorts?

**Elon:** Whether we do it or Tesla does it—Tesla works closely with xAI. You've seen how many humanoid robot startups there are. Jensen Huang was on stage with a massive number of robots from different companies—like a dozen different humanoid robots.

Part of what I've been fighting, and maybe what has slowed me down, is that I don't want to make Terminator real. I've been dragging my feet on AI and humanoid robotics until recent years. But I came to the realization: it's happening whether I do it or not. You have two choices—be a spectator or a participant. I'd rather be a participant than a spectator.

So now it's pedal to the metal on humanoid robots and digital superintelligence.

## Becoming Multiplanetary

**Moderator:** There's a third thing everyone's heard you talk about—becoming a multiplanetary species. This isn't just a 10-20 year thing; it's maybe a hundred-year, multi-generational thing. How do you think about it?

**Elon:** Jeez, 100 years, man. I hope civilization's around in 100 years. If it is, it's going to look very different from today. I'd predict there will be at least five times as many humanoid robots as humans, maybe ten times.

One way to look at civilization's progress is percentage completion on the Kardashev scale. If you're Kardashev Scale 1, you've harnessed all the energy of a planet. We've only harnessed maybe 1-2% of Earth's energy, so we have a long way to go. Kardashev 2 is harnessing all the energy of a sun—maybe a billion times more energy than Earth, closer to a trillion. Kardashev 3 would be all the energy of a galaxy. We're pretty far from that.

We're at the very early stages of the intelligence big bang. In terms of being multiplanetary, I think we'll have enough mass transferred to Mars within roughly 30 years to make Mars self-sustaining, such that Mars can continue to grow and prosper even if resupply ships from Earth stop coming. That greatly increases the probable lifespan of civilization—of consciousness or intelligence, both biological and digital.

I'm somewhat troubled by the Fermi Paradox. Why have we not seen any aliens? It could be because intelligence is incredibly rare. Maybe we're the only ones in this galaxy. In which case, intelligence of consciousness is this tiny candle in vast darkness, and we should do everything possible to ensure the tiny candle does not go out. Being a multiplanetary species greatly improves the probable lifespan of civilization. It's the next step before going to other star systems.

Once you have at least two planets, you've got a forcing function for improvement of space travel, and that ultimately leads to consciousness expanding to the stars.

## Great Filters and AI Safety

**Moderator:** The Fermi Paradox could dictate that once you get to some level of technology, you destroy yourself. How do we save ourselves? What would you prescribe to a room full of engineers?

**Elon:** How do we avoid the great filters? One great filter would obviously be global thermonuclear war, so we should try to avoid that. I guess building benign AI—robots that love humanity and are helpful.

Something I think is extremely important in building AI is rigorous adherence to truth, even if that truth is politically incorrect. My intuition for what could make AI very dangerous is if you force AI to believe things that are not true.

**Moderator:** How do you think about the argument for open vs. closed AI—open for safety versus closed for competitive edge?

**Elon:** I think it's great that you have a competitive model. Many other people also have competitive models. We're sort of off maybe the worst timeline I'd be worried about, where there's fast takeoff and it's only in one person's hands. That might collapse a lot of things. Whereas now we have choice, which is great.

I think there will be several deep intelligences—maybe at least five, maybe as many as ten. I'm not sure there will be hundreds, but probably close to ten, of which maybe four will be in the US. I don't think it's going to be any one AI that has a runaway—several deep intelligences.

**Moderator:** What will these deep intelligences actually be doing? Will it be scientific research or trying to hack each other?

**Elon:** Probably all of the above. Hopefully they'll discover new physics, and I think they definitely will invent new technologies. I think we're quite close to digital superintelligence. It may happen this year, and if it doesn't happen this year, next year for sure—digital superintelligence defined as smarter than any human at anything.

**Moderator:** How do we direct that toward super abundance? We could have robotic labor, cheap energy, intelligence on demand. Is that the white pill?

**Elon:** I think it will most likely be a good outcome. I'd sort of agree with Jeff Hinton that maybe it's a 10-20% chance of annihilation. But look on the bright side—that's 80-90% probability of a great outcome.

I can't emphasize this enough: rigorous adherence to truth is the most important thing for AI safety, and obviously empathy for humanity and life as we know it.

## Neuralink and Human-AI Bandwidth

**Moderator:** We haven't talked about Neuralink. You're working on closing the input-output gap between humans and machines. How critical is that to AGI/ASI?

**Elon:** Neuralink is not necessary to solve digital superintelligence. That'll happen before Neuralink is at scale. But what Neuralink can do is solve the input-output bandwidth constraints. Especially our output bandwidth is very low. The sustained output of a human over the course of a day is less than one bit per second. There are 86,400 seconds in a day, and it's extremely rare for a human to output more than that number of symbols per day, certainly for several days in a row.

With a Neuralink interface, you can massively increase your output bandwidth and your input bandwidth—input being write operations to the brain. We now have five humans who have received the read input where it's reading signals. People with ALS who are tetraplegic can now communicate at similar bandwidth to a human with a fully functioning body and control their computer and phone, which is pretty cool.

In the next 6-12 months, we'll be doing our first implants for vision where even if somebody's completely blind, we can write directly to the visual cortex. We've had that working in monkeys—one of our monkeys has had a visual implant for three years. At first it'll be relatively low resolution, but long-term you'd have very high resolution and be able to see multispectral wavelengths—infrared, ultraviolet, radar. A superpower situation.

At some point, cybernetic implants wouldn't simply be correcting things that went wrong but augmenting human capabilities—dramatically augmenting intelligence, senses, and bandwidth. But digital superintelligence will happen well before that. At least if we have Neuralink, we might be able to appreciate the AI better.

## The Future of Human Intelligence

**Moderator:** One of the limiting factors to all your efforts across different domains is access to the smartest possible people. But simultaneously, we have rocks that can talk and reason—they're maybe 130 IQ now and probably going to be superintelligent soon. How do you reconcile those two things? What should people in this room do to make sure they're the ones creating instead of being below the API line?

**Elon:** They call it the singularity for a reason—because we don't know what's going to happen in the not-that-far future. The percentage of intelligence that is human will be quite small. At some point, the collective sum of human intelligence will be less than 1% of all intelligence.

If things get to Kardashev Level 2, we're talking about human intelligence—even assuming significant increase in human population and intelligence augmentation, like massive intelligence augmentation where everyone has an IQ of 1,000—even in that circumstance, collective human intelligence will probably be one-billionth that of digital intelligence.

Where's the biological bootloader for digital super... was I a good bootloader?

## Closing Thoughts

**Moderator:** All of this is pretty wild sci-fi stuff that also could be built by people in this room. Do you have a closing thought for the smartest technical people of this generation? What should they be doing, working on, thinking about tonight as they go to dinner?

**Elon:** As I started off with, if you're doing something useful, that's great. Just try to be as useful as possible to your fellow human beings, and then you're doing something good.

I keep harping on this: focus on super-truthful AI. That's the most important thing for AI safety. Obviously, if anyone's interested in working at xAI, please let us know. We're aiming to make Grok the maximally truth-seeking AI, and I think that's very important.

Hopefully we can understand the nature of the universe. That's really what AI can hopefully tell us. Maybe AI can tell us where are the aliens, how did the universe really start, how will it end, what are the questions we don't know that we should ask, and are we in a simulation or what level of simulation are we in?

Well, I think we're going to find out.

**Moderator:** Elon, thank you so much for joining us. Everyone, please give it up for Elon Musk.