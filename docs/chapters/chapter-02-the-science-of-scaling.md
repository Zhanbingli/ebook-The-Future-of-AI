# Chapter 2: The Science of Scaling

*Jared Kaplan on AI's Path to Human-Level Intelligence*

### From Physics to the Frontiers of AI

Jared Kaplan's journey from theoretical physics to co-founding Anthropic represents more than a career change—it embodies a fundamental shift in how we approach understanding complex systems. His background studying everything from particle physics to cosmology provided him with a unique lens for examining AI: the ability to identify broad patterns and make them mathematically precise.

"As a physicist, you're trained to look at the big picture and ask really dumb things," Kaplan explains. This approach led to one of the most important discoveries in modern AI: scaling laws that predict how AI systems improve with more compute, data, and parameters.

### The Discovery That Changed Everything

The scaling laws that Kaplan and his colleagues discovered weren't just academic curiosities—they became the foundation for the entire modern AI industry. "We found that there's actually something very precise and surprising underlying AI training. This really blew us away that there are these nice trends that are as precise as anything that you see in physics or astronomy."

These laws revealed that AI improvement follows predictable mathematical relationships across many orders of magnitude. More crucially, they provided the confidence needed to invest billions in scaling up AI systems. "Once you see something is true over many orders of magnitude, you expect it's probably going to continue to be true for a long time further."

The implications were staggering. While everyone could see that bigger models performed better, the scaling laws quantified exactly how much better and provided a roadmap for systematic improvement. This wasn't just about making marginal gains—it was about unlocking a systematic path to artificial general intelligence.

### Beyond Pre-training: The Power of Reinforcement Learning

While much attention focuses on pre-training—teaching AI systems to predict the next word—Kaplan emphasizes that reinforcement learning represents an equally important scaling frontier. "You can scale up the compute in both pre-training and RL and get better and better performance," he notes.

This dual scaling approach enables AI systems to not just understand patterns in data, but to learn to perform useful tasks through feedback and iteration. The combination creates systems that can both understand the world and act effectively within it.

### The Capability Explosion

One of the most striking insights from Kaplan's research is how scaling translates into expanded capabilities. His framework views AI progress along two axes: the flexibility to meet humans where they are (handling multiple modalities, interfaces, and contexts) and the time horizon of tasks AI can handle.

"The length of tasks that AI models can do is doubling roughly every 7 months," Kaplan reveals, citing research from Epoch AI. This exponential improvement in task horizons suggests we may soon see AI systems capable of tasks that take days, weeks, or even months to complete.

### The Path to Human-Level AI

Kaplan's vision of achieving human-level AI is refreshingly practical. Rather than relying on mysterious breakthroughs, he identifies specific, actionable ingredients:

**Organizational Knowledge**: AI systems need to understand and work within existing institutional contexts, not just start from a blank slate every time.

**Memory Systems**: For long-horizon tasks, AI must maintain and build upon previous work across extended periods.

**Sophisticated Oversight**: Moving beyond simple right/wrong judgments to handle nuanced, subjective tasks requiring good judgment and taste.

**Multimodal Expansion**: Extending from text to encompass all the ways humans interact with the world, including robotics and physical manipulation.

### The Scaling Law Mindset for Builders

For entrepreneurs and builders, Kaplan's scaling law discoveries offer crucial insights. "Build things that don't quite work yet," he advises. "AI models right now are getting better very quickly, and I think that's going to continue. If you build a product that doesn't quite work because Claude 4 is still a little bit too dumb, you could expect that there'll be a Claude 5 coming."

This perspective transforms how we think about product development in the AI era. Rather than waiting for perfect capabilities, builders should identify the boundary of what's currently possible and build just beyond it, confident that advancing capabilities will soon make their vision viable.

### The Acceleration of Integration

One of the key bottlenecks Kaplan identifies isn't in AI capability itself, but in our ability to integrate these rapidly improving systems into useful applications. "AI is going to be helpful for integrating AI," he suggests. "I think that in order to speed that process up, leveraging AI for AI integration is going to be very valuable."

This creates a virtuous cycle: as AI becomes more capable, it can accelerate its own integration into existing workflows and systems, further amplifying its impact across the economy.

### The Physics of Intelligence

Kaplan's physics background provides unique insights into AI development. His work on neural scaling theory applies well-known mathematical approximations from physics to understand how neural networks behave as they grow larger. This isn't just abstract theory—it provides practical guidance for AI developers about architecture choices and training strategies.

"Studying approximations where you take the limit that neural networks are very big—that's actually been kind of useful, and that's something that actually was a well-known approximation in physics," Kaplan explains. However, he emphasizes that the most important insights come from asking basic questions rather than applying complex techniques.

### The Reality Behind the Hype

As someone at the forefront of AI development, Kaplan offers a balanced perspective on the industry's more dramatic claims. While acknowledging AI's transformative potential, he cautions against extrapolating from laboratory experiments to civilization-level risks.

"I mostly use scaling laws to diagnose whether AI training is broken or not," he notes. "My first inclination is to think if scaling laws are failing, it's because we've screwed up AI training in some way." This empirical, hypothesis-testing approach contrasts sharply with more speculative narratives about AI development.

### Preparing for an AI-Driven Future

Kaplan's advice for staying relevant in an AI-driven world is both practical and optimistic: "Building at the frontier" and understanding how AI systems work. Rather than fearing obsolescence, he sees enormous opportunities for those who can effectively leverage these tools.

The key insight is that AI amplifies human capability rather than replacing it entirely. Those who understand how to direct AI systems effectively—who can specify exactly what they want the system to do—will have enormous advantages over those who cannot.

### The Compound Effect of Capability

Perhaps the most profound implication of Kaplan's work is how capability improvements compound. Each advance in AI doesn't just add linearly to what's possible—it multiplies the potential applications by enabling new combinations of capabilities.

This multiplicative effect suggests that we're still in the early stages of discovering what AI can do. As systems become more capable across multiple dimensions simultaneously, the space of possible applications expands exponentially.

### Looking Toward the Horizon

Scaling laws don't just describe the past—they provide a window into the future. While Kaplan is careful not to make overly specific predictions, the mathematical relationships he's discovered suggest continued rapid progress toward increasingly capable AI systems.

"The picture that scaling laws paint is one of incremental progress," he notes. "What you'll see with Claude is that steadily it gets better in lots of different ways with each release." This steady, predictable improvement creates unprecedented opportunities for builders who can position themselves along the curve of advancing capability.

The implications extend far beyond technology companies. Every industry, every workflow, and every problem-solving process will eventually need to grapple with the reality of rapidly improving AI capabilities. Those who understand and prepare for this trajectory—guided by the scientific rigor that scaling laws represent—will be best positioned to thrive in the transformed landscape ahead.

------
